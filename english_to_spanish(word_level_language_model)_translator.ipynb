{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qe4n0y_He3Km"
      },
      "outputs": [],
      "source": [
        "from word_level_transformer import Transformer\n",
        "import torch\n",
        "import numpy as np\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FNF865MhfEdI"
      },
      "outputs": [],
      "source": [
        "START_TOKEN = 'S'\n",
        "PADDING_TOKEN = 'P'\n",
        "END_TOKEN = 'E'\n",
        "\n",
        "# spanish_vocabulary = [START_TOKEN,END_TOKEN,PADDING_TOKEN,'?','.','!',',','¿','a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'ñ', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',' ']\n",
        "# english_vocabulary = [START_TOKEN,END_TOKEN,PADDING_TOKEN,'?','.','!',',','¿','a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'ñ', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',' ']\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1pBj3ma7hVc4"
      },
      "outputs": [],
      "source": [
        "file_path = '/content/drive/MyDrive/spa.txt'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ge2rvFA0yZuR"
      },
      "outputs": [],
      "source": [
        "def text_preprocessing(text):\n",
        "  text = re.sub(r\"([?.!,¿])\", r\" \\1 \", text)\n",
        "  text = re.sub(r'[\" \"]+', \" \", text)\n",
        "  text = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", text)\n",
        "  text = text.strip().lower()\n",
        "\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ViD2B7hgyuKx"
      },
      "outputs": [],
      "source": [
        "num_data = 80000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8zWU4rSJik85"
      },
      "outputs": [],
      "source": [
        "with open(file_path,'r') as f:\n",
        "  lines = f.readlines()\n",
        "english_sentence,spanish_sentence = [],[]\n",
        "for total_example,line in enumerate(lines):\n",
        "  if (total_example < num_data ):\n",
        "    line = line.lower()\n",
        "    data = line.replace(\"\\n\",\"\").split(\"\\t\")\n",
        "    data[0] = text_preprocessing(data[0])\n",
        "    data[1] = text_preprocessing(data[1])\n",
        "    english_sentence.append(data[0])\n",
        "    spanish_sentence.append(data[1])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "S6x1jFuOR7tn"
      },
      "outputs": [],
      "source": [
        "spanish_vocabulary = [START_TOKEN,PADDING_TOKEN,END_TOKEN]\n",
        "english_vocabulary = [START_TOKEN,PADDING_TOKEN,END_TOKEN]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "t_3PTj5uLsiw"
      },
      "outputs": [],
      "source": [
        "for en,sp in zip(english_sentence,spanish_sentence):\n",
        "  en_tokens = en.split()\n",
        "  sp_tokens = sp.split()\n",
        "  for en_token,sp_token in zip(en_tokens,sp_tokens):\n",
        "    if en_token not in english_vocabulary:\n",
        "      english_vocabulary.append(en_token)\n",
        "    if sp_token not in spanish_vocabulary:\n",
        "      spanish_vocabulary.append(sp_token)\n",
        "  # print(english_vocabulary)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "hLMxoXfchi6W"
      },
      "outputs": [],
      "source": [
        "index_to_spanish = {index:word for index,word in enumerate(spanish_vocabulary)}\n",
        "spanish_to_index = {word:index for index,word in enumerate(spanish_vocabulary)}\n",
        "index_to_english = {index:word for index,word in enumerate(english_vocabulary)}\n",
        "english_to_index = {word:index for index,word in enumerate(english_vocabulary)}\n",
        "# print(spanish_to_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8Dv2I1FOKub",
        "outputId": "d2cf9e19-4312-4992-d6ce-a99bdb15c9f8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(9116, 15968)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(english_vocabulary),len(spanish_vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkLWejZ3bcXn"
      },
      "outputs": [],
      "source": [
        "english_vocabulary = english_vocabulary[:70000]\n",
        "spanish_vocabulary = spanish_vocabulary[:70000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0gpYOyFkQFy",
        "outputId": "c926e979-aba5-4052-9747-66a2fef53991"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['go .',\n",
              " 'go .',\n",
              " 'go .',\n",
              " 'go .',\n",
              " 'hi .',\n",
              " 'run !',\n",
              " 'run .',\n",
              " 'who ?',\n",
              " 'fire !',\n",
              " 'fire !']"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "english_sentence[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dv7VWb8GkWHh",
        "outputId": "d7d21637-7aa5-4695-cdee-192ab2cfbb1b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['ve .',\n",
              " 'vete .',\n",
              " 'vaya .',\n",
              " 'v yase .',\n",
              " 'hola .',\n",
              " 'corre !',\n",
              " 'corred .',\n",
              " '¿ qui n ?',\n",
              " 'fuego !',\n",
              " 'incendio !']"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spanish_sentence[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYljiEZkkiA0",
        "outputId": "cc54a4ee-e5f8-4914-cef1-da74b2fd3850"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(38, 71)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max(len(x) for x in english_sentence),max(len(x) for x in spanish_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1g1mTQwkwt6",
        "outputId": "fa2b01e0-b6f6-4612-cfe2-96dd7a532885"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25.3962125\n",
            "26.6830875\n"
          ]
        }
      ],
      "source": [
        "# computing avg length\n",
        "print(sum(len(x) for x in english_sentence)/len(english_sentence))\n",
        "print(sum(len(x) for x in spanish_sentence)/len(spanish_sentence))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ms85Y28mn3bV"
      },
      "outputs": [],
      "source": [
        "def is_valid_length(sentence,max_sequence_length):\n",
        "  return len(sentence) < (max_sequence_length-2) # we want to add the start and end token by managing the sequence_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVPAtFLJoJtv",
        "outputId": "bb21fe7c-a583-4742-be22-d8934f2562a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "is_valid_length('i am',7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "9sl5aqM4oJnS"
      },
      "outputs": [],
      "source": [
        "def truncate_sentences(lng_sentence,max_sequence_length):\n",
        "  for i,sentence in enumerate(lng_sentence):\n",
        "    checker = is_valid_length(sentence,max_sequence_length)\n",
        "    if not checker: ## if legth is greater then (max-sequence-length minus 2)\n",
        "      lng_sentence[i] = sentence[:max_sequence_length-2]\n",
        "  return lng_sentence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "vtca6awt9J_z"
      },
      "outputs": [],
      "source": [
        "d_model = 152\n",
        "batch_size = 32\n",
        "ffn_hidden = 256\n",
        "num_heads = 2\n",
        "drop_prob = 0.1\n",
        "num_stacked = 1\n",
        "max_sequence_length = 35\n",
        "sp_vocab_size = len(spanish_vocabulary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkbD6SsvsbYv"
      },
      "source": [
        "limit the length of the sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Ko9KvYQxqmSj"
      },
      "outputs": [],
      "source": [
        "english_sentence = truncate_sentences(english_sentence,max_sequence_length)\n",
        "spanish_sentence = truncate_sentences(spanish_sentence,max_sequence_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhCxoQqzq4o3",
        "outputId": "aadedf64-a12f-4e3c-8d36-0041da1b800f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(33, 33)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max(len(x) for x in english_sentence),max(len(x) for x in spanish_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "O5am0HOPuWHz"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "  '''\n",
        "  overriding certain methods of the Dataset class\n",
        "  '''\n",
        "  def __init__(self,english_sentence,spanish_sentence):\n",
        "    self.english_sentence = english_sentence\n",
        "    self.spanish_sentence = spanish_sentence\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.english_sentence)\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    return self.english_sentence[index],self.spanish_sentence[index]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Y0q7IGjZvVEH"
      },
      "outputs": [],
      "source": [
        "dataset = TextDataset(english_sentence,spanish_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eI5iOYbvX77",
        "outputId": "8d5cb888-ea94-4924-ab95-dd3fb7195dd0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "80000"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "KRLIKa_tv_CJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_loader = DataLoader(dataset,batch_size=batch_size)\n",
        "iterator = iter(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBTgtR9cwUSA",
        "outputId": "a3d9e728-8c7d-4aec-be2e-2431baf86c5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('go .', 'go .', 'go .', 'go .', 'hi .', 'run !', 'run .', 'who ?', 'fire !', 'fire !', 'fire !', 'help !', 'help !', 'help !', 'jump !', 'jump .', 'stop !', 'stop !', 'stop !', 'wait !', 'wait .', 'go on .', 'go on .', 'hello !', 'i ran .', 'i ran .', 'i try .', 'i won !', 'oh no !', 'relax .', 'smile .', 'attack !'), ('ve .', 'vete .', 'vaya .', 'v yase .', 'hola .', 'corre !', 'corred .', '¿ qui n ?', 'fuego !', 'incendio !', 'disparad !', 'ayuda !', 'socorro ! auxilio !', 'auxilio !', 'salta !', 'salte .', 'parad !', 'para !', 'pare !', 'espera !', 'esperen .', 'contin a .', 'contin e .', 'hola .', 'corr .', 'corr a .', 'lo intento .', 'he ganado !', 'oh , no !', 'tom telo con soda .', 'sonr e .', 'al ataque !')]\n",
            "[('attack !', 'get up .', 'go now .', 'got it !', 'got it ?', 'got it ?', 'he ran .', 'hop in .', 'hug me .', 'i fell .', 'i know .', 'i left .', 'i lied .', 'i lost .', 'i quit .', 'i quit .', 'i work .', 'i m .', 'i m up .', 'listen .', 'listen .', 'listen .', 'no way !', 'no way !', 'no way !', 'no way !', 'no way !', 'no way !', 'no way !', 'no way !', 'no way !', 'no way !'), ('atacad !', 'levanta .', 've ahora mismo .', 'lo tengo !', '¿ lo pillas ?', '¿ entendiste ?', 'l corri .', 'm tete adentro .', 'abr zame .', 'me ca .', 'yo lo s .', 'sal .', 'ment .', 'perd .', 'dimito .', 'renunci .', 'estoy trabajando .', 'tengo diecinueve .', 'estoy levantado .', 'escucha .', 'escuche .', 'escuchen .', 'no puede ser !', 'de ninguna manera .', 'de ninguna manera !', 'imposible !', 'de ning n modo !', 'de eso nada !', 'ni cagando !', 'mangos !', 'minga !', 'ni en pedo !')]\n",
            "[('really ?', 'really ?', 'thanks .', 'thanks .', 'try it .', 'we try .', 'we won .', 'why me ?', 'ask tom .', 'awesome !', 'be calm .', 'be cool .', 'be fair .', 'be kind .', 'be nice .', 'beat it .', 'call me .', 'call me .', 'call me .', 'call us .', 'come in .', 'come in .', 'come in .', 'come on !', 'come on .', 'come on .', 'drop it !', 'get tom .', 'get out !', 'get out .', 'get out .', 'get out .'), ('¿ en serio ?', '¿ la verdad ?', 'gracias !', 'gracias .', 'pru balo .', 'lo procuramos .', 'ganamos .', '¿ por qu yo ?', 'preg ntale a tom .', 'rale !', 'mantente en calma .', 'estate tranquilo .', 's justo .', 'sean gentiles .', 's agradable .', 'p rate .', 'llamame .', 'llamadme .', 'll mame .', 'll manos .', 'entre .', 'pase .', 'entren !', 'rale !', 'ndale .', 'rale !', 'su ltalo .', 'agarra a tom .', 'b jate .', 'salte .', 'sal .', 'sal .')]\n",
            "[('get out .', 'get out .', 'go away !', 'go away !', 'go away !', 'go away !', 'go away !', 'go away !', 'go away !', 'go away .', 'go away .', 'go away .', 'go away .', 'go away .', 'go away .', 'go away .', 'go home .', 'go slow .', 'goodbye !', 'goodbye !', 'goodbye !', 'hang on !', 'hang on !', 'hang on !', 'hang on .', 'he came .', 'he quit .', 'help me !', 'help me .', 'help me .', 'help me .', 'help us .'), ('salid .', 'salgan .', 'vete de aqu !', 'l rgate !', 'salga de aqu !', 'largo !', 'vete ya !', 'rale !', 'a la calle !', 'vete de aqu !', 'l rgate !', 'largo !', 'vete ya !', 'rale !', 'l rguese .', 'v yase .', 'vete a casa .', 'vaya despacio .', 'hasta luego !', 'hasta la vista .', 'chau !', 'espera !', 'espera un momento !', 'un segundo !', 'agarra fuertemente .', 'l vino .', 'l renunci .', 'ay dame .', 'ay dame .', 'chame una mano .', 'ayudame .', 'ay danos .')]\n",
            "[('hit tom .', 'hold it !', 'hold on .', 'hold on .', 'hold on .', 'hug tom .', 'i agree .', 'i agree .', 'i bowed .', 'i moved .', 'i moved .', 'i moved .', 'i moved .', 'i slept .', 'i tried .', 'i ll go .', 'i m tom .', 'i m fat .', 'i m fat .', 'i m fit .', 'i m hit !', 'i m old .', 'i m shy .', 'i m wet .', 'it s ok .', 'it s me !', 'it s me .', 'join us .', 'join us .', 'keep it .', 'me , too .', 'open up .'), ('golpea a tom .', 'espera .', 'resista .', 'resiste .', 'agarra fuertemente .', 'abraza a tom .', 'estoy de acuerdo .', 'de acuerdo .', 'me inclin .', 'me he mudado .', 'me mud .', 'me traslad .', 'me he trasladado .', 'dorm .', 'lo intent .', 'ir .', 'soy tom .', 'estoy gordo .', 'soy gorda .', 'estoy en forma .', 'estoy afectado .', 'soy viejo .', 'soy t mido .', 'estoy mojada .', 'est bien .', 'soy yo .', 'soy yo .', 'nete a nosotros .', 's parte nuestra .', 'gu rdalo .', 'yo , tambi n .', 'abre .')]\n"
          ]
        }
      ],
      "source": [
        "for batch_num,batch in enumerate(iterator):\n",
        "  print(batch)\n",
        "  # break\n",
        "  if (batch_num > 3):\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "eVHzNQWAA8g2"
      },
      "outputs": [],
      "source": [
        "transformer = Transformer(d_model,\n",
        "                          ffn_hidden,\n",
        "                          num_heads,\n",
        "                          drop_prob,\n",
        "                          num_stacked,\n",
        "                          max_sequence_length,\n",
        "                          sp_vocab_size,\n",
        "                          english_to_index,\n",
        "                          spanish_to_index,\n",
        "                          START_TOKEN,\n",
        "                          END_TOKEN,\n",
        "                          PADDING_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkQmg8k5DCXh",
        "outputId": "db2a6944-c6f4-4702-dd31-b2ee35fd7a9d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (encoder): Encoder(\n",
              "    (sentence_embedding): SentenceEmbedding(\n",
              "      (embedding): Embedding(9116, 152)\n",
              "      (position_encoder): AbsolutePositionalEncoding()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (layers): SequentialEncoder(\n",
              "      (0): EncoderLayer(\n",
              "        (attention): MultiHeadAttention(\n",
              "          (qkv_layer): Linear(in_features=152, out_features=456, bias=True)\n",
              "          (linear_layer): Linear(in_features=152, out_features=152, bias=True)\n",
              "        )\n",
              "        (norm1): LayerNormalization()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (ffn): PositionalwiseFeedForwrd(\n",
              "          (linear1): Linear(in_features=152, out_features=256, bias=True)\n",
              "          (linear2): Linear(in_features=256, out_features=152, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (norm2): LayerNormalization()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (sentence_embedding): SentenceEmbedding(\n",
              "      (embedding): Embedding(15968, 152)\n",
              "      (position_encoder): AbsolutePositionalEncoding()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (layers): SequentialDecoder(\n",
              "      (0): DecoderLayer(\n",
              "        (masked_attention): MultiHeadAttention(\n",
              "          (qkv_layer): Linear(in_features=152, out_features=456, bias=True)\n",
              "          (linear_layer): Linear(in_features=152, out_features=152, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (norm1): LayerNormalization()\n",
              "        (encoder_decoder_attention): MultiHeadCrossAttention(\n",
              "          (kv_layer): Linear(in_features=152, out_features=304, bias=True)\n",
              "          (q_layer): Linear(in_features=152, out_features=152, bias=True)\n",
              "          (linear_layer): Linear(in_features=152, out_features=152, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNormalization()\n",
              "        (ffn): PositionalwiseFeedForwrd(\n",
              "          (linear1): Linear(in_features=152, out_features=256, bias=True)\n",
              "          (linear2): Linear(in_features=256, out_features=152, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (norm3): LayerNormalization()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (linear): Linear(in_features=152, out_features=15968, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "meuR2adX-h8y"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "criterian = nn.CrossEntropyLoss(ignore_index=spanish_to_index[PADDING_TOKEN],\n",
        "                                reduction='none')\n",
        "\n",
        "# When computing the loss, we are ignoring cases when the label is the padding token\n",
        "for params in transformer.parameters():\n",
        "    if params.dim() > 1:\n",
        "        nn.init.xavier_uniform_(params)\n",
        "\n",
        "optim = torch.optim.Adam(transformer.parameters(), lr=1e-4)\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "GTV2rZfRDk15"
      },
      "outputs": [],
      "source": [
        "NEG_INFTY = -1e9\n",
        "\n",
        "def create_masks(eng_batch, sp_batch):\n",
        "    num_sentences = len(eng_batch) # {represent batch size}\n",
        "    look_ahead_mask = torch.full([max_sequence_length, max_sequence_length] , True)\n",
        "    look_ahead_mask = torch.triu(look_ahead_mask, diagonal=1)\n",
        "    encoder_padding_mask = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
        "    decoder_padding_mask_self_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
        "    decoder_padding_mask_cross_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
        "\n",
        "    for idx in range(num_sentences):\n",
        "      eng_sentence_length, sp_sentence_length = len(eng_batch[idx]), len(sp_batch[idx])\n",
        "      eng_chars_to_padding_mask = np.arange(eng_sentence_length + 1, max_sequence_length)\n",
        "      sp_chars_to_padding_mask = np.arange(sp_sentence_length + 1, max_sequence_length)\n",
        "      encoder_padding_mask[idx, :, eng_chars_to_padding_mask] = True\n",
        "      encoder_padding_mask[idx, eng_chars_to_padding_mask, :] = True\n",
        "      decoder_padding_mask_self_attention[idx, :, sp_chars_to_padding_mask] = True\n",
        "      decoder_padding_mask_self_attention[idx, sp_chars_to_padding_mask, :] = True\n",
        "      decoder_padding_mask_cross_attention[idx, :, eng_chars_to_padding_mask] = True\n",
        "      decoder_padding_mask_cross_attention[idx, sp_chars_to_padding_mask, :] = True\n",
        "\n",
        "    encoder_self_attention_mask = torch.where(encoder_padding_mask, NEG_INFTY, 0)\n",
        "    decoder_self_attention_mask =  torch.where(look_ahead_mask + decoder_padding_mask_self_attention, NEG_INFTY, 0)\n",
        "    decoder_cross_attention_mask = torch.where(decoder_padding_mask_cross_attention, NEG_INFTY, 0)\n",
        "    return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8kyA6OsJWKA"
      },
      "source": [
        "Explaination 1 :\n",
        "\n",
        "encoder_self_attention_mask and decoder_cross_attention_mask are used so that transformer do not pay attention to the padding tokens (which is done by putting zeros (till the length of the sentence + 1) and remaning part in sequence is covered by -infinity)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        example:\n",
        "        max_sequence_length  = 8,   \n",
        "                  sentence =  'good'(len = 4),num_sentence = 1\n",
        "                  zero values should be till index len(sentence) + 1\n",
        "\n",
        "\n",
        "        mask = [[[0 0 0 0 0 -inf -inf -inf],\n",
        "                 [0 0 0 0 0 -inf -inf -inf],\n",
        "                 [0 0 0 0 0 -inf -inf -inf],\n",
        "                 [0 0 0 0 0 -inf -inf -inf],\n",
        "                 [0 0 0 0 0 -inf -inf -inf],\n",
        "                 [0 0 0 0 0 -inf -inf -inf],\n",
        "                 [0 0 0 0 0 -inf -inf -inf],\n",
        "                 [0 0 0 0 0 -inf -inf -inf]]]\n",
        "                shape (num_sentence,max_sequence_length,max_sequence_length)\n",
        "        \n",
        "        attention_weights = [[[0.00426842, 0.00752416, 0.00349225, 0.00898395, 0.00556792, -0.00568639, -0.00325177, 0.00724002],\n",
        "                             [0.00921758, 0.00367077, 0.00327958, 0.00263405, 0.00764168, -0.00392446, -0.00628386, 0.00685052],\n",
        "                             [0.00997227, 0.00228168, 0.00833329, 0.00146394, 0.00922879, -0.00393896, -0.00372312, 0.00919514],\n",
        "                             [0.00588389, 0.00815015, 0.00625498, 0.00393098, 0.0071409, -0.00682445, -0.00449244, 0.00170309],\n",
        "                             [0.00125849, 0.00692958, 0.00917532, 0.00639848, 0.00209307, -0.00023777, -0.00540265, 0.00118428],\n",
        "                             [0.00485591, 0.00195363, 0.00936389, 0.00918742, 0.00358588, -0.00993245, -0.00042846, 0.00660049],\n",
        "                             [0.00730663, 0.00275739, 0.00828811, 0.00286777, 0.00250849, -0.00248524, -0.00326519, 0.00197197],\n",
        "                             [0.00901291, 0.00702945, 0.00767226, 0.00873171, 0.0090118, -0.00064111, -0.00999714, 0.00365651]]]\n",
        "                             shape (batch_size = 1,max_sequence_length,max_sequence_length)\n",
        "\n",
        "\n",
        "        result =  (mask + attention_weights)\n",
        "                         [[[0.00426842, 0.00752416, 0.00349225, 0.00898395, 0.00556792, -inf, -inf, -inf],\n",
        "                         [0.00921758, 0.00367077, 0.00327958, 0.00263405, 0.00764168, -inf, -inf, -inf],\n",
        "                         [0.00997227, 0.00228168, 0.00833329, 0.00146394, 0.00922879, -inf, -inf, -inf],\n",
        "                         [0.00588389, 0.00815015, 0.00625498, 0.00393098, 0.0071409, -inf, -inf,-inf],\n",
        "                         [0.00125849, 0.00692958, 0.00917532, 0.00639848, 0.00209307, -inf, -inf, -inf],\n",
        "                         [0.00485591, 0.00195363, 0.00936389, 0.00918742, 0.00358588, -inf, -inf,-inf],\n",
        "                         [0.00730663, 0.00275739, 0.00828811, 0.00286777, 0.00250849, -inf, -inf, -inf],\n",
        "                         [0.00901291, 0.00702945, 0.00767226, 0.00873171, 0.0090118, -inf, -inf, -inf]]]\n",
        "            \n",
        "\n",
        "\n",
        "        softmax(result,dim = -1)\n",
        "                  [[[0.1997, 0.2003, 0.1995, 0.2006, 0.1999, 0.0000, 0.0000, 0.0000],\n",
        "                    [0.2008, 0.1997, 0.1996, 0.1995, 0.2005, 0.0000, 0.0000, 0.0000],\n",
        "                    [0.2007, 0.1992, 0.2004, 0.1990, 0.2006, 0.0000, 0.0000, 0.0000],\n",
        "                    [0.1999, 0.2004, 0.2000, 0.1995, 0.2002, 0.0000, 0.0000, 0.0000],\n",
        "                    [0.1992, 0.2004, 0.2008, 0.2002, 0.1994, 0.0000, 0.0000, 0.0000],\n",
        "                    [0.1998, 0.1992, 0.2007, 0.2007, 0.1996, 0.0000, 0.0000, 0.0000],\n",
        "                    [0.2005, 0.1996, 0.2007, 0.1996, 0.1996, 0.0000, 0.0000, 0.0000],\n",
        "                    [0.2001, 0.1997, 0.1999, 0.2001, 0.2001, 0.0000, 0.0000, 0.0000]]]\n",
        "\n",
        "                         \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Explaination 2 :\n",
        "\n",
        "decoder_self_attention_mask is mainly used by the decoder's first sublayer known as masked_self_attention which is used so that while producing target token ,decoder should not able to see(or attend)  the future token or words\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      example:\n",
        "        max_sequence_length  = 8,   \n",
        "                  sentence =  'good'(len = 4),num_sentence = 1\n",
        "                  zero values should be till index len(sentence) + 1\n",
        "\n",
        "\n",
        "        mask = [[[0 -inf -inf -inf -inf -inf -inf -inf],\n",
        "                 [0  0  -inf  -inf -inf -inf -inf -inf],\n",
        "                 [0  0   0   -inf  -inf -inf -inf -inf],\n",
        "                 [0  0   0   0  -inf -inf -inf -inf],\n",
        "                 [0  0   0   0   0 -inf -inf -inf],\n",
        "                 [-inf -inf -inf -inf -inf  -inf -inf -inf],\n",
        "                 [-inf -inf -inf -inf -inf -inf -inf -inf],\n",
        "                 [-inf -inf -inf -inf -inf -inf -inf -inf ]]\n",
        "                shape (num_sentence,max_sequence_length,max_sequence_length)\n",
        "\n",
        "\n",
        "                Last  few vectors are filled  with infinity values\n",
        "                because zeros have filled the total index which they can fill\n",
        "                that is (len(good) + 1) and after that all is padding token where we do not need to pay attention that's why after paying attention to the last token of the sequence ,the next rows are filled with -infinity\n",
        "\n",
        "                & Similar above steps..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jn6vcch0F9gu",
        "outputId": "d90f4079-f7a5-4b84-fb4f-80a6155bfc81"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 35, 35])"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eng_batch = (\"happy\",'good','joyful','sri ram jai ram')\n",
        "sp_batch = (\"happy\",'good','joyful','hbhwsd')\n",
        "\n",
        "encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(eng_batch,sp_batch)\n",
        "encoder_self_attention_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6mxDm4iGzW5"
      },
      "outputs": [],
      "source": [
        "encoder_self_attention_mask[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCZNSw677ERg"
      },
      "outputs": [],
      "source": [
        "decoder_self_attention_mask[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VD8i3sis7Nck"
      },
      "outputs": [],
      "source": [
        "decoder_cross_attention_mask[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HHcp31W_wNp"
      },
      "outputs": [],
      "source": [
        "decoder_self_attention_mask[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwdwXFz4CYFZ"
      },
      "outputs": [],
      "source": [
        "transformer.train() # used to set the model in training mode\n",
        "total_loss = 0\n",
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "  iterator = iter(train_loader)\n",
        "  '''\n",
        "  looping on iterator we will get the batch of input_sequence and target_sequence\n",
        "  [(batch_of_input_sequence),(batch_of_target_sequence)]\n",
        "  '''\n",
        "\n",
        "  for batch_num,batch  in enumerate(iterator):\n",
        "    '''\n",
        "    batch_num{int}: current batch number\n",
        "    batch{list{tuple}}: batch of input_sequence and target_sequence\n",
        "    '''\n",
        "    transformer.train()\n",
        "    eng_batch,sp_batch = batch\n",
        "    # creating the mask for the eng_batch and sp_batch\n",
        "    encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(eng_batch, sp_batch)\n",
        "    '''\n",
        "    ***important***\n",
        "    {the gradients of each mini-batch should be computed independantly}\n",
        "\n",
        "    .zero-grad():set gradient to zero at the start of training of each mini-batch\n",
        "                 so that while backpropogation the gradient is not accumulated:\n",
        "                              (means gradient of these mini batch would not effect the gradient of other mini-batch)\n",
        "\n",
        "    '''\n",
        "    optim.zero_grad()\n",
        "\n",
        "    sp_predictions = transformer(eng_batch,\n",
        "                                 sp_batch,\n",
        "                                 encoder_self_attention_mask,\n",
        "                                 decoder_self_attention_mask,\n",
        "                                 decoder_cross_attention_mask,\n",
        "                                 enc_start_token = False,\n",
        "                                 enc_end_token = False,\n",
        "                                 dec_start_token = True,\n",
        "                                 dec_end_token = True)\n",
        "    '''\n",
        "    labels{tensor}:\n",
        "          converting spanish sentences into their index values based on spanish_to_index\n",
        "    '''\n",
        "\n",
        "    labels = transformer.decoder.sentence_embedding.batch_tokenize(sp_batch, start_token=False, end_token=True) # shspe (batch_size,max_sequence_length) {max_sequence_length  = num_queries}\n",
        "\n",
        "    '''\n",
        "    loss = criterian(....):\n",
        "        represent loss of each mini-batch\n",
        "        by computimg loss over all characters(or tokens) in a batch of sentences\n",
        "        loss[:max_sequnce_length] = loss of 1st sentence of batch\n",
        "    '''\n",
        "    loss = criterian(\n",
        "        sp_predictions.view(-1,sp_vocab_size).to(device), # shape (batch_size * num_queries,sp_vocab_size)\n",
        "        labels.view(-1).to(device) # shape (bach_size * num_queries.)\n",
        "    ) # shape (batch_size * num_queries)\n",
        "  \n",
        "\n",
        "    '''\n",
        "    valid_indices:\n",
        "                setting true value where labels are not padding token\n",
        "                and false value where labels are padding token\n",
        "    '''\n",
        "    valid_indicies = torch.where(labels.view(-1) == spanish_to_index[PADDING_TOKEN], False, True) # shape (batch_size * num_queries)\n",
        "    # print(valid_indicies[:50])\n",
        "    # print(loss.sum())\n",
        "    # print(valid_indicies.sum())\n",
        "    '''\n",
        "    loss = loss.sum()/valid...:\n",
        "                represent a loss value(single number), where the loss of all padding tokens are ignored\n",
        "\n",
        "    '''\n",
        "    loss = loss.sum() / valid_indicies.sum()\n",
        "\n",
        "\n",
        "    '''\n",
        "    loss.backward():\n",
        "                compute gradient or derivative using the loss function and model's parameters\n",
        "                *** Theory ***\n",
        "                          L = loss function\n",
        "                          w = model's weight or pparameter\n",
        "                          dL/dw = gradient of loss function wrt weight w\n",
        "                          dL/dw:\n",
        "                                represent how the loss function is changing if we change the model's parameter w(increase or decrease w value)\n",
        "\n",
        "                                computed using chain rule of calculus\n",
        "\n",
        "    optim.step():\n",
        "              updating the model parameter with above computed gradient or derivative\n",
        "              using a specific optmizer equation{Adam,Gradient descent,SGD,...}\n",
        "\n",
        "    .item():\n",
        "            return the value of torch tensor which containe only one value\n",
        "\n",
        "    '''\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "\n",
        "    if (batch_num % 100 == 0):\n",
        "      print(f\"epoch {epoch} batch {batch_num} loss {loss.item()}\")\n",
        "      print(f\"English: {eng_batch[0]}\")\n",
        "      print(f\"Spanish Translation: {sp_batch[0]}\")\n",
        "      '''\n",
        "      sp_predictions[0]:\n",
        "                  represent the output of transformer for a first trainable example in a batch\n",
        "      sp_sentence_predicted:\n",
        "                  represent the predicted index of spanish sentence for a first trainable example in a batch\n",
        "      '''\n",
        "\n",
        "      sp_sentence_predicted = torch.argmax(sp_predictions[0], # shape (num_queries,sp_vocab_size)\n",
        "                                           axis=1)  # shape (num_queries,)\n",
        "\n",
        "      predicted_sentence = \"\"\n",
        "      for idx in sp_sentence_predicted:\n",
        "        # print(idx.item())\n",
        "        if idx == spanish_to_index[END_TOKEN]:\n",
        "          break\n",
        "        predicted_sentence += index_to_spanish[idx.item()]\n",
        "      print(f'Spaniish Prediction: {predicted_sentence}')\n",
        "\n",
        "\n",
        "      transformer.eval()  # used to set the model in evaluation mode\n",
        "      '''\n",
        "      ***Important***\n",
        "         a =  ('dksdkk')\n",
        "         type(a),len(a)\n",
        "         --> str, 6\n",
        "         a =  ('dksdkk',)\n",
        "         type(a),len(a)\n",
        "         ---> tuple, 1\n",
        "      '''\n",
        "      sp_sentence = (\"\",)\n",
        "      eng_sentence  = [\"everyone is happy.\"]\n",
        "      for i,sentence in enumerate(eng_sentence):\n",
        "        eng_sentence[i] = text_preprocessing(sentence)\n",
        "      eng_sentence = tuple(eng_sentence)\n",
        "      for word_counter in range(max_sequence_length):\n",
        "          encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask= create_masks(eng_sentence, sp_sentence)\n",
        "          predictions = transformer(eng_sentence,\n",
        "                                          sp_sentence,\n",
        "                                          encoder_self_attention_mask,\n",
        "                                          decoder_self_attention_mask,\n",
        "                                          decoder_cross_attention_mask,\n",
        "                                          enc_start_token=False,\n",
        "                                          enc_end_token=False,\n",
        "                                          dec_start_token=True,\n",
        "                                          dec_end_token=False)\n",
        "        \n",
        "          '''\n",
        "          next_token_:\n",
        "                  contains raw prediction values for each token in vocabulary\n",
        "                  example:\n",
        "                        \"<start>i am happy<end>\"\n",
        "                        input token <start> feeded to transformer decoder\n",
        "                        then output of transformer represent raw prediction values,\n",
        "                        predicting what could be the next word (after <start> token) form the vocabulary\n",
        "                        shape (sp_vocab_size,)\n",
        "\n",
        "                                  I    <end>   a      m     h\n",
        "                        output = [21.5 , 7.9  , 0.1 , -20 , -2.4 ,...] {random_values}\n",
        "                        highest prediction value is the next word\n",
        "\n",
        "                        {real raw values range depnds on type activation function that is used}\n",
        "\n",
        "\n",
        "          '''\n",
        "          next_token_raw_distribution = predictions[0][word_counter] # shape (sp_vocab_size,)\n",
        "          next_token_index = torch.argmax(next_token_raw_distribution).item()\n",
        "          next_token = index_to_spanish[next_token_index]\n",
        "          sp_sentence = (sp_sentence[0] + next_token,)\n",
        "          if (next_token == END_TOKEN):\n",
        "            break\n",
        "\n",
        "      print(f'Evaluation translation {eng_sentence[0]} : {sp_sentence[0]}')\n",
        "      print(f'-----------------------------------------------------------')\n",
        "\n",
        "\n",
        "    # break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1Si_wsPRtz_"
      },
      "outputs": [],
      "source": [
        "# english_to_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaGJriaTNIRo"
      },
      "outputs": [],
      "source": [
        "transformer.eval()\n",
        "def translate(input_sentence):\n",
        "  '''\n",
        "      ***Important***\n",
        "         a =  ('dksdkk')\n",
        "         type(a),len(a)\n",
        "         --> str, 6\n",
        "         a =  ('dksdkk',)\n",
        "         type(a),len(a)\n",
        "         ---> tuple, 1\n",
        "      '''\n",
        "  sp_sentence = (\"\",)\n",
        "  eng_sentence  = [input_sentence]\n",
        "  for i,sentence in enumerate(eng_sentence):\n",
        "      eng_sentence[i] = text_preprocessing(sentence)\n",
        "  eng_sentence = tuple(eng_sentence)\n",
        "  for word_counter in range(max_sequence_length):\n",
        "      encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask= create_masks(eng_sentence, sp_sentence)\n",
        "      predictions = transformer(eng_sentence,\n",
        "                                          sp_sentence,\n",
        "                                          encoder_self_attention_mask,\n",
        "                                          decoder_self_attention_mask,\n",
        "                                          decoder_cross_attention_mask,\n",
        "                                          enc_start_token=False,\n",
        "                                          enc_end_token=False,\n",
        "                                          dec_start_token=True,\n",
        "                                          dec_end_token=False)\n",
        "      print(predictions.shape)\n",
        "      '''\n",
        "      next_token_:\n",
        "              contains raw prediction values for each token in vocabulary\n",
        "              example:\n",
        "                    \"<start>i am happy<end>\"\n",
        "                    input token <start> feeded to transformer decoder\n",
        "                    then output of transformer represent raw prediction values,\n",
        "                    predicting what could be the next word (after <start> token) form the vocabulary\n",
        "                    shape (sp_vocab_size,)\n",
        "\n",
        "                              I    <end>   a      m     h\n",
        "                    output = [21.5 , 7.9  , 0.1 , -20 , -2.4 ,...] {random_values}\n",
        "                    highest prediction value is the next word\n",
        "\n",
        "                    {real raw values range depnds on type activation function that is used}\n",
        "\n",
        "\n",
        "      '''\n",
        "      next_token_raw_distribution = predictions[0][word_counter] # shape (sp_vocab_size,)\n",
        "      # print(next_token_raw_distribution)\n",
        "      next_token_index = torch.argmax(next_token_raw_distribution).item()\n",
        "      # print(next_token_index)\n",
        "      next_token = index_to_spanish[next_token_index]\n",
        "      # print(next_token)\n",
        "      sp_sentence = (sp_sentence[0] + next_token,)\n",
        "      if (next_token == END_TOKEN):\n",
        "        break\n",
        "\n",
        "  print(f'Evaluation translation {eng_sentence[0]} : {sp_sentence[0]}')\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
