{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qe4n0y_He3Km"
      },
      "outputs": [],
      "source": [
        "from word_level_transformer import Transformer\n",
        "import torch\n",
        "import numpy as np\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNF865MhfEdI"
      },
      "outputs": [],
      "source": [
        "START_TOKEN = 'S'\n",
        "PADDING_TOKEN = 'P'\n",
        "END_TOKEN = 'E'\n",
        "\n",
        "spanish_vocabulary = [START_TOKEN,END_TOKEN,PADDING_TOKEN,'?','.','!',',','¿','a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'ñ', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',' ']\n",
        "english_vocabulary = [START_TOKEN,END_TOKEN,PADDING_TOKEN,'?','.','!',',','¿','a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'ñ', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',' ']\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pBj3ma7hVc4"
      },
      "outputs": [],
      "source": [
        "file_path = '/content/drive/MyDrive/spa.txt'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLMxoXfchi6W"
      },
      "outputs": [],
      "source": [
        "index_to_spanish = {index:word for index,word in enumerate(spanish_vocabulary)}\n",
        "spanish_to_index = {word:index for index,word in enumerate(spanish_vocabulary)}\n",
        "index_to_english = {index:word for index,word in enumerate(english_vocabulary)}\n",
        "english_to_index = {word:index for index,word in enumerate(english_vocabulary)}\n",
        "# print(spanish_to_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ge2rvFA0yZuR"
      },
      "outputs": [],
      "source": [
        "def text_preprocessing(text):\n",
        "  text = re.sub(r\"([?.!,¿])\", r\" \\1 \", text)\n",
        "  text = re.sub(r'[\" \"]+', \" \", text)\n",
        "  text = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", text)\n",
        "  text = text.strip().lower()\n",
        "\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ViD2B7hgyuKx"
      },
      "outputs": [],
      "source": [
        "num_data = 80000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zWU4rSJik85"
      },
      "outputs": [],
      "source": [
        "with open(file_path,'r') as f:\n",
        "  lines = f.readlines()\n",
        "english_sentence,spanish_sentence = [],[]\n",
        "for total_example,line in enumerate(lines):\n",
        "  if (total_example < num_data ):\n",
        "    line = line.lower()\n",
        "    data = line.replace(\"\\n\",\"\").split(\"\\t\")\n",
        "    data[0] = text_preprocessing(data[0])\n",
        "    data[1] = text_preprocessing(data[1])\n",
        "    english_sentence.append(data[0])\n",
        "    spanish_sentence.append(data[1])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0gpYOyFkQFy"
      },
      "outputs": [],
      "source": [
        "english_sentence[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dv7VWb8GkWHh"
      },
      "outputs": [],
      "source": [
        "spanish_sentence[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYljiEZkkiA0"
      },
      "outputs": [],
      "source": [
        "max(len(x) for x in english_sentence),max(len(x) for x in spanish_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1g1mTQwkwt6"
      },
      "outputs": [],
      "source": [
        "# computing avg length\n",
        "print(sum(len(x) for x in english_sentence)/len(english_sentence))\n",
        "print(sum(len(x) for x in spanish_sentence)/len(spanish_sentence))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ms85Y28mn3bV"
      },
      "outputs": [],
      "source": [
        "def is_valid_length(sentence,max_sequence_length):\n",
        "  return len(sentence) < (max_sequence_length-2) # we want to add the start and end token by managing the sequence_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVPAtFLJoJtv"
      },
      "outputs": [],
      "source": [
        "is_valid_length('i am',7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sl5aqM4oJnS"
      },
      "outputs": [],
      "source": [
        "def truncate_sentences(lng_sentence,max_sequence_length):\n",
        "  for i,sentence in enumerate(lng_sentence):\n",
        "    checker = is_valid_length(sentence,max_sequence_length)\n",
        "    if not checker: ## if legth is greater then (max-sequence-length minus 2)\n",
        "      lng_sentence[i] = sentence[:max_sequence_length-2]\n",
        "  return lng_sentence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtca6awt9J_z"
      },
      "outputs": [],
      "source": [
        "d_model = 256\n",
        "batch_size = 32\n",
        "ffn_hidden = 256\n",
        "num_heads = 8\n",
        "drop_prob = 0.1\n",
        "num_stacked = 1\n",
        "max_sequence_length = 50\n",
        "sp_vocab_size = len(spanish_vocabulary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkbD6SsvsbYv"
      },
      "source": [
        "limit the length of the sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ko9KvYQxqmSj"
      },
      "outputs": [],
      "source": [
        "english_sentence = truncate_sentences(english_sentence,max_sequence_length)\n",
        "spanish_sentence = truncate_sentences(spanish_sentence,max_sequence_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhCxoQqzq4o3"
      },
      "outputs": [],
      "source": [
        "max(len(x) for x in english_sentence),max(len(x) for x in spanish_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5am0HOPuWHz"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "  '''\n",
        "  overriding certain methods of the Dataset class\n",
        "  '''\n",
        "  def __init__(self,english_sentence,spanish_sentence):\n",
        "    self.english_sentence = english_sentence\n",
        "    self.spanish_sentence = spanish_sentence\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.english_sentence)\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    return self.english_sentence[index],self.spanish_sentence[index]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0q7IGjZvVEH"
      },
      "outputs": [],
      "source": [
        "dataset = TextDataset(english_sentence,spanish_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9eI5iOYbvX77"
      },
      "outputs": [],
      "source": [
        "len(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRLIKa_tv_CJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_loader = DataLoader(dataset,batch_size=batch_size)\n",
        "iterator = iter(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBTgtR9cwUSA"
      },
      "outputs": [],
      "source": [
        "for batch_num,batch in enumerate(iterator):\n",
        "  print(batch)\n",
        "  # break\n",
        "  if (batch_num > 3):\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVHzNQWAA8g2"
      },
      "outputs": [],
      "source": [
        "transformer = Transformer(d_model,\n",
        "                          ffn_hidden,\n",
        "                          num_heads,\n",
        "                          drop_prob,\n",
        "                          num_stacked,\n",
        "                          max_sequence_length,\n",
        "                          sp_vocab_size,\n",
        "                          english_to_index,\n",
        "                          spanish_to_index,\n",
        "                          START_TOKEN,\n",
        "                          END_TOKEN,\n",
        "                          PADDING_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkQmg8k5DCXh",
        "outputId": "a3c15a08-6216-4533-942e-7356f79475a6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (encoder): Encoder(\n",
              "    (sentence_embedding): SentenceEmbedding(\n",
              "      (embedding): Embedding(36, 256)\n",
              "      (position_encoder): AbsolutePositionalEncoding()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (layers): SequentialEncoder(\n",
              "      (0): EncoderLayer(\n",
              "        (attention): MultiHeadAttention(\n",
              "          (qkv_layer): Linear(in_features=256, out_features=768, bias=True)\n",
              "          (linear_layer): Linear(in_features=256, out_features=256, bias=True)\n",
              "        )\n",
              "        (norm1): LayerNormalization()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (ffn): PositionalwiseFeedForwrd(\n",
              "          (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (norm2): LayerNormalization()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (sentence_embedding): SentenceEmbedding(\n",
              "      (embedding): Embedding(36, 256)\n",
              "      (position_encoder): AbsolutePositionalEncoding()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (layers): SequentialDecoder(\n",
              "      (0): DecoderLayer(\n",
              "        (masked_attention): MultiHeadAttention(\n",
              "          (qkv_layer): Linear(in_features=256, out_features=768, bias=True)\n",
              "          (linear_layer): Linear(in_features=256, out_features=256, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (norm1): LayerNormalization()\n",
              "        (encoder_decoder_attention): MultiHeadCrossAttention(\n",
              "          (kv_layer): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (q_layer): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (linear_layer): Linear(in_features=256, out_features=256, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNormalization()\n",
              "        (ffn): PositionalwiseFeedForwrd(\n",
              "          (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (norm3): LayerNormalization()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (linear): Linear(in_features=256, out_features=36, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meuR2adX-h8y"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "criterian = nn.CrossEntropyLoss(ignore_index=spanish_to_index[PADDING_TOKEN],\n",
        "                                reduction='none')\n",
        "\n",
        "# When computing the loss, we are ignoring cases when the label is the padding token\n",
        "for params in transformer.parameters():\n",
        "    if params.dim() > 1:\n",
        "        nn.init.xavier_uniform_(params)\n",
        "\n",
        "optim = torch.optim.Adam(transformer.parameters(), lr=1e-4)\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTV2rZfRDk15"
      },
      "outputs": [],
      "source": [
        "NEG_INFTY = -1e9\n",
        "\n",
        "def create_masks(eng_batch, sp_batch):\n",
        "    num_sentences = len(eng_batch) # {represent batch size}\n",
        "    look_ahead_mask = torch.full([max_sequence_length, max_sequence_length] , True)\n",
        "    look_ahead_mask = torch.triu(look_ahead_mask, diagonal=1)\n",
        "    encoder_padding_mask = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
        "    decoder_padding_mask_self_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
        "    decoder_padding_mask_cross_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
        "\n",
        "    for idx in range(num_sentences):\n",
        "      eng_sentence_length, sp_sentence_length = len(eng_batch[idx]), len(sp_batch[idx])\n",
        "      eng_chars_to_padding_mask = np.arange(eng_sentence_length + 1, max_sequence_length)\n",
        "      sp_chars_to_padding_mask = np.arange(sp_sentence_length + 1, max_sequence_length)\n",
        "      encoder_padding_mask[idx, :, eng_chars_to_padding_mask] = True\n",
        "      encoder_padding_mask[idx, eng_chars_to_padding_mask, :] = True\n",
        "      decoder_padding_mask_self_attention[idx, :, sp_chars_to_padding_mask] = True\n",
        "      decoder_padding_mask_self_attention[idx, sp_chars_to_padding_mask, :] = True\n",
        "      decoder_padding_mask_cross_attention[idx, :, eng_chars_to_padding_mask] = True\n",
        "      decoder_padding_mask_cross_attention[idx, sp_chars_to_padding_mask, :] = True\n",
        "\n",
        "    encoder_self_attention_mask = torch.where(encoder_padding_mask, NEG_INFTY, 0)\n",
        "    decoder_self_attention_mask =  torch.where(look_ahead_mask + decoder_padding_mask_self_attention, NEG_INFTY, 0)\n",
        "    decoder_cross_attention_mask = torch.where(decoder_padding_mask_cross_attention, NEG_INFTY, 0)\n",
        "    return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8kyA6OsJWKA"
      },
      "source": [
        "Explaination 1 :\n",
        "\n",
        "encoder_self_attention_mask and decoder_cross_attention_mask are used so that transformer do not pay attention to the padding tokens (which is done by putting zeros (till the length of the sentence + 1) and remaning part in sequence is covered by -infinity)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        example:\n",
        "        max_sequence_length  = 8,   \n",
        "                  sentence =  'good'(len = 4),num_sentence = 1\n",
        "                  zero values should be till index len(sentence) + 1\n",
        "\n",
        "\n",
        "        mask = [[[0 0 0 0 0 -inf -inf -inf],\n",
        "                 [0 0 0 0 0 -inf -inf -inf],\n",
        "                 [0 0 0 0 0 -inf -inf -inf],\n",
        "                 [0 0 0 0 0 -inf -inf -inf],\n",
        "                 [0 0 0 0 0 -inf -inf -inf],\n",
        "                 [0 0 0 0 0 -inf -inf -inf],\n",
        "                 [0 0 0 0 0 -inf -inf -inf],\n",
        "                 [0 0 0 0 0 -inf -inf -inf]]]\n",
        "                shape (num_sentence,max_sequence_length,max_sequence_length)\n",
        "        \n",
        "        attention_weights = [[[0.00426842, 0.00752416, 0.00349225, 0.00898395, 0.00556792, -0.00568639, -0.00325177, 0.00724002],\n",
        "                             [0.00921758, 0.00367077, 0.00327958, 0.00263405, 0.00764168, -0.00392446, -0.00628386, 0.00685052],\n",
        "                             [0.00997227, 0.00228168, 0.00833329, 0.00146394, 0.00922879, -0.00393896, -0.00372312, 0.00919514],\n",
        "                             [0.00588389, 0.00815015, 0.00625498, 0.00393098, 0.0071409, -0.00682445, -0.00449244, 0.00170309],\n",
        "                             [0.00125849, 0.00692958, 0.00917532, 0.00639848, 0.00209307, -0.00023777, -0.00540265, 0.00118428],\n",
        "                             [0.00485591, 0.00195363, 0.00936389, 0.00918742, 0.00358588, -0.00993245, -0.00042846, 0.00660049],\n",
        "                             [0.00730663, 0.00275739, 0.00828811, 0.00286777, 0.00250849, -0.00248524, -0.00326519, 0.00197197],\n",
        "                             [0.00901291, 0.00702945, 0.00767226, 0.00873171, 0.0090118, -0.00064111, -0.00999714, 0.00365651]]]\n",
        "                             shape (batch_size = 1,max_sequence_length,max_sequence_length)\n",
        "\n",
        "\n",
        "        result =  (mask + attention_weights)\n",
        "                         [[[0.00426842, 0.00752416, 0.00349225, 0.00898395, 0.00556792, -inf, -inf, -inf],\n",
        "                         [0.00921758, 0.00367077, 0.00327958, 0.00263405, 0.00764168, -inf, -inf, -inf],\n",
        "                         [0.00997227, 0.00228168, 0.00833329, 0.00146394, 0.00922879, -inf, -inf, -inf],\n",
        "                         [0.00588389, 0.00815015, 0.00625498, 0.00393098, 0.0071409, -inf, -inf,-inf],\n",
        "                         [0.00125849, 0.00692958, 0.00917532, 0.00639848, 0.00209307, -inf, -inf, -inf],\n",
        "                         [0.00485591, 0.00195363, 0.00936389, 0.00918742, 0.00358588, -inf, -inf,-inf],\n",
        "                         [0.00730663, 0.00275739, 0.00828811, 0.00286777, 0.00250849, -inf, -inf, -inf],\n",
        "                         [0.00901291, 0.00702945, 0.00767226, 0.00873171, 0.0090118, -inf, -inf, -inf]]]\n",
        "            \n",
        "\n",
        "\n",
        "        softmax(result,dim = -1)\n",
        "                  [[[0.1997, 0.2003, 0.1995, 0.2006, 0.1999, 0.0000, 0.0000, 0.0000],\n",
        "                    [0.2008, 0.1997, 0.1996, 0.1995, 0.2005, 0.0000, 0.0000, 0.0000],\n",
        "                    [0.2007, 0.1992, 0.2004, 0.1990, 0.2006, 0.0000, 0.0000, 0.0000],\n",
        "                    [0.1999, 0.2004, 0.2000, 0.1995, 0.2002, 0.0000, 0.0000, 0.0000],\n",
        "                    [0.1992, 0.2004, 0.2008, 0.2002, 0.1994, 0.0000, 0.0000, 0.0000],\n",
        "                    [0.1998, 0.1992, 0.2007, 0.2007, 0.1996, 0.0000, 0.0000, 0.0000],\n",
        "                    [0.2005, 0.1996, 0.2007, 0.1996, 0.1996, 0.0000, 0.0000, 0.0000],\n",
        "                    [0.2001, 0.1997, 0.1999, 0.2001, 0.2001, 0.0000, 0.0000, 0.0000]]]\n",
        "\n",
        "                         \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Explaination 2 :\n",
        "\n",
        "decoder_self_attention_mask is mainly used by the decoder's first sublayer known as masked_self_attention which is used so that while producing target token ,decoder should not able to see(or attend)  the future token or words\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      example:\n",
        "        max_sequence_length  = 8,   \n",
        "                  sentence =  'good'(len = 4),num_sentence = 1\n",
        "                  zero values should be till index len(sentence) + 1\n",
        "\n",
        "\n",
        "        mask = [[[0 -inf -inf -inf -inf -inf -inf -inf],\n",
        "                 [0  0  -inf  -inf -inf -inf -inf -inf],\n",
        "                 [0  0   0   -inf  -inf -inf -inf -inf],\n",
        "                 [0  0   0   0  -inf -inf -inf -inf],\n",
        "                 [0  0   0   0   0 -inf -inf -inf],\n",
        "                 [-inf -inf -inf -inf -inf  -inf -inf -inf],\n",
        "                 [-inf -inf -inf -inf -inf -inf -inf -inf],\n",
        "                 [-inf -inf -inf -inf -inf -inf -inf -inf ]]\n",
        "                shape (num_sentence,max_sequence_length,max_sequence_length)\n",
        "\n",
        "\n",
        "                Last  few vectors are filled  with infinity values\n",
        "                because zeros have filled the total index which they can fill\n",
        "                that is (len(good) + 1) and after that all is padding token where we do not need to pay attention that's why after paying attention to the last token of the sequence ,the next rows are filled with -infinity\n",
        "\n",
        "                & Similar above steps..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jn6vcch0F9gu"
      },
      "outputs": [],
      "source": [
        "eng_batch = (\"happy\",'good','joyful','sri ram jai ram')\n",
        "sp_batch = (\"happy\",'good','joyful','hbhwsd')\n",
        "\n",
        "encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(eng_batch,sp_batch)\n",
        "encoder_self_attention_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6mxDm4iGzW5"
      },
      "outputs": [],
      "source": [
        "encoder_self_attention_mask[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCZNSw677ERg"
      },
      "outputs": [],
      "source": [
        "decoder_self_attention_mask[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VD8i3sis7Nck"
      },
      "outputs": [],
      "source": [
        "decoder_cross_attention_mask[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HHcp31W_wNp"
      },
      "outputs": [],
      "source": [
        "decoder_self_attention_mask[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwdwXFz4CYFZ",
        "outputId": "a3d371ea-9ce3-4bcc-fdfd-4226d004962f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0 batch 0 loss 4.555313587188721\n",
            "English: go .\n",
            "Spanish Translation: ve .\n",
            "Spaniish Prediction: kfbqbqqbjbPqqkbbbbnddgjbz¿bbbbbhhbbbomkkkukmohhhhh\n",
            "Evaluation translation everyone is happy . : bqqqqqjbbjjjbkbbbbkddbbbbbbbbbbhhhhhbmkklsqqlhhhhx\n",
            "-----------------------------------------------------------\n",
            "epoch 0 batch 100 loss 2.8594970703125\n",
            "English: she stood up .\n",
            "Spanish Translation: ella se levant .\n",
            "Spaniish Prediction: so   oto no o . .  \n",
            "Evaluation translation everyone is happy . :               .E\n",
            "-----------------------------------------------------------\n",
            "epoch 0 batch 200 loss 2.5722219944000244\n",
            "English: it s tom s job .\n",
            "Spanish Translation: es la tarea de tom .\n",
            "Spaniish Prediction: no  oe o    ar \n",
            "Evaluation translation everyone is happy . : s   e  a      .E\n",
            "-----------------------------------------------------------\n",
            "epoch 0 batch 300 loss 2.8601043224334717\n",
            "English: why are you sad ?\n",
            "Spanish Translation: ¿ por qu est s triste ?\n",
            "Spaniish Prediction: eot  os  e  ea saoa\n",
            "Evaluation translation everyone is happy . : s      a    a . o .E\n",
            "-----------------------------------------------------------\n",
            "epoch 0 batch 400 loss 2.6113908290863037\n",
            "English: did you know that ?\n",
            "Spanish Translation: ¿ sab as eso ?\n",
            "Spaniish Prediction: ¿s  ees  e  a \n",
            "Evaluation translation everyone is happy . : s   ee e e e aE\n",
            "-----------------------------------------------------------\n",
            "epoch 0 batch 500 loss 2.568073034286499\n",
            "English: he changed his job .\n",
            "Spanish Translation: l cambi de empleo .\n",
            "Spaniish Prediction: ¿       e  e      o\n",
            "Evaluation translation everyone is happy . : n e e e  e e aE\n",
            "-----------------------------------------------------------\n",
            "epoch 0 batch 600 loss 2.602473497390747\n",
            "English: can he speak french ?\n",
            "Spanish Translation: ¿ l habla franc s ?\n",
            "Spaniish Prediction: t m te ee se ae\n",
            "Evaluation translation everyone is happy . : n e es es e esE\n",
            "-----------------------------------------------------------\n",
            "epoch 0 batch 700 loss 2.6496667861938477\n",
            "English: what is on the desk ?\n",
            "Spanish Translation: ¿ qu hay en el escritorio ?\n",
            "Spaniish Prediction: tom eee ue  a aen or a o\n",
            "Evaluation translation everyone is happy . : m m m ma a e a .E\n",
            "-----------------------------------------------------------\n",
            "epoch 0 batch 800 loss 2.209199905395508\n",
            "English: tom is mary s mentor .\n",
            "Spanish Translation: tom es el mentor de mary .\n",
            "Spaniish Prediction: eo aa  esneen e  ces.o o .\n",
            "Evaluation translation everyone is happy . : l l e e c e e .E\n",
            "-----------------------------------------------------------\n",
            "epoch 0 batch 900 loss 2.419072151184082\n",
            "English: put it where you like .\n",
            "Spanish Translation: p nganlo donde quieran .\n",
            "Spaniish Prediction: lom    a ae  e t e sa  .\n",
            "Evaluation translation everyone is happy . : n m e a a a a .E\n",
            "-----------------------------------------------------------\n",
            "epoch 0 batch 1000 loss 2.2980737686157227\n",
            "English: i really got depressed .\n",
            "Spanish Translation: me sent verdaderamente deprimido .\n",
            "Spaniish Prediction: eene  eeee  e es  as aste oo aeo .\n",
            "Evaluation translation everyone is happy . : n m e e co e e .E\n",
            "-----------------------------------------------------------\n",
            "epoch 0 batch 1100 loss 2.167506217956543\n",
            "English: don t play with matches .\n",
            "Spanish Translation: no juegues con f sforos .\n",
            "Spaniish Prediction: ¿  duesu s du  p . ala  .\n",
            "Evaluation translation everyone is happy . : n ques pa a es .E\n",
            "-----------------------------------------------------------\n",
            "epoch 0 batch 1200 loss 2.289238452911377\n",
            "English: we re having lunch here .\n",
            "Spanish Translation: estamos almorzando aqu .\n",
            "Spaniish Prediction: est     m aasoa  a . u .\n",
            "Evaluation translation everyone is happy . :  m m m a m m a .E\n",
            "-----------------------------------------------------------\n",
            "epoch 0 batch 1300 loss 2.1239538192749023\n",
            "English: she will be back at five .\n",
            "Spanish Translation: estar de vuelta a las cinco .\n",
            "Spaniish Prediction: es o  ee eeenea a ae  eo ea .\n",
            "Evaluation translation everyone is happy . : n pa a a la a .E\n",
            "-----------------------------------------------------------\n",
            "epoch 0 batch 1400 loss 1.9888153076171875\n",
            "English: i want to eat some grapes .\n",
            "Spanish Translation: quiero comerme unas uvas .\n",
            "Spaniish Prediction: nu  no pe ese  ena  . en n\n",
            "Evaluation translation everyone is happy . : no eno co ca e .E\n",
            "-----------------------------------------------------------\n",
            "epoch 0 batch 1500 loss 2.0760035514831543\n",
            "English: he is a cheerful young man .\n",
            "Spanish Translation: es un joven alegre .\n",
            "Spaniish Prediction: estee no istt e    .\n",
            "Evaluation translation everyone is happy . :  l e e cu po e .E\n",
            "-----------------------------------------------------------\n",
            "epoch 0 batch 1600 loss 1.820558786392212\n",
            "English: why did you tell this joke ?\n",
            "Spanish Translation: ¿ por qu contaste esa broma ?\n",
            "Spaniish Prediction: ¿ m   su p   ast  e  se a   .\n",
            "Evaluation translation everyone is happy . :  m s s m m m m .E\n",
            "-----------------------------------------------------------\n",
            "epoch 0 batch 1700 loss 1.7970775365829468\n",
            "English: they re washing their hands .\n",
            "Spanish Translation: ellos se est n lavando las manos .\n",
            "Spaniish Prediction: el a  sesestie da adoo a n .a o  .\n",
            "Evaluation translation everyone is happy . : l la la l cie .E\n",
            "-----------------------------------------------------------\n",
            "epoch 0 batch 1800 loss 1.8296051025390625\n",
            "English: i ve already eaten breakfast .\n",
            "Spanish Translation: ya he desayunado .\n",
            "Spaniish Prediction: lo qo qestd etro .\n",
            "Evaluation translation everyone is happy . : o quero quirr .E\n",
            "-----------------------------------------------------------\n",
            "epoch 0 batch 1900 loss 1.607094645500183\n",
            "English: i couldn t get my car started .\n",
            "Spanish Translation: no pude poner mi coche en marcha .\n",
            "Spaniish Prediction: nompuee poner virco oa e es naaa .\n",
            "Evaluation translation everyone is happy . :  l te l cicie .E\n",
            "-----------------------------------------------------------\n",
            "epoch 0 batch 2000 loss 1.5830355882644653\n",
            "English: do you know when she will come ?\n",
            "Spanish Translation: ¿ sabes cu ndo vendr ?\n",
            "Spaniish Prediction: ¿ mu es ci ndo nendea.\n",
            "Evaluation translation everyone is happy . :  que qu cirrr .E\n",
            "-----------------------------------------------------------\n",
            "epoch 0 batch 2100 loss 1.7315961122512817\n",
            "English: we are sorry we can t help you .\n",
            "Spanish Translation: lo sentimos mucho por no poder ayudarlos .\n",
            "Spaniish Prediction: loss sticossm cto ao  d  ee e e.  eer o  .\n",
            "Evaluation translation everyone is happy . : m m ste m mie .E\n",
            "-----------------------------------------------------------\n",
            "epoch 0 batch 2200 loss 1.6060374975204468\n",
            "English: the sun always sets in the west .\n",
            "Spanish Translation: el sol siempre se pone en el oeste .\n",
            "Spaniish Prediction: eloso eseemure se pe   p  nn s  to .\n",
            "Evaluation translation everyone is happy . :  po to hompomero .E\n",
            "-----------------------------------------------------------\n",
            "epoch 0 batch 2300 loss 1.7123998403549194\n",
            "English: please hurry . we re late already .\n",
            "Spanish Translation: haga el favor de apresurarse ya llegamos tarde .\n",
            "Spaniish Prediction: ma a e ahalor de apo s     asm el  susa  .a   s.\n",
            "Evaluation translation everyone is happy . :  m qun qubllo .E\n",
            "-----------------------------------------------------------\n",
            "epoch 0 batch 2400 loss 1.4120124578475952\n",
            "English: i ve been in boston since october .\n",
            "Spanish Translation: he estado desde octubre en boston .\n",
            "Spaniish Prediction: me es edo des erosuaea reso.a  a  .\n",
            "Evaluation translation everyone is happy . :  p quu qubbllabababa .E\n",
            "-----------------------------------------------------------\n",
            "epoch 1 batch 0 loss 2.469543933868408\n",
            "English: go .\n",
            "Spanish Translation: ve .\n",
            "Spaniish Prediction: p  p\n",
            "Evaluation translation everyone is happy . :  pomir qubblo .E\n",
            "-----------------------------------------------------------\n",
            "epoch 1 batch 100 loss 1.038621187210083\n",
            "English: she stood up .\n",
            "Spanish Translation: ella se levant .\n",
            "Spaniish Prediction: el abs  levant .\n",
            "Evaluation translation everyone is happy . : m poy mpbbbbi .E\n",
            "-----------------------------------------------------------\n",
            "epoch 1 batch 200 loss 0.7804492712020874\n",
            "English: it s tom s job .\n",
            "Spanish Translation: es la tarea de tom .\n",
            "Spaniish Prediction: essha taraa do tom .\n",
            "Evaluation translation everyone is happy . :  poyoy .E\n",
            "-----------------------------------------------------------\n",
            "epoch 1 batch 300 loss 0.8411465883255005\n",
            "English: why are you sad ?\n",
            "Spanish Translation: ¿ por qu est s triste ?\n",
            "Spaniish Prediction: ¿ por au est n tris a .\n",
            "Evaluation translation everyone is happy . : poyoy pomommmabpppo .E\n",
            "-----------------------------------------------------------\n",
            "epoch 1 batch 400 loss 0.7255005240440369\n",
            "English: did you know that ?\n",
            "Spanish Translation: ¿ sab as eso ?\n",
            "Spaniish Prediction: ¿ sa ass eso d\n",
            "Evaluation translation everyone is happy . :  lloyo mimmii .E\n",
            "-----------------------------------------------------------\n",
            "epoch 1 batch 500 loss 0.8048150539398193\n",
            "English: he changed his job .\n",
            "Spanish Translation: l cambi de empleo .\n",
            "Spaniish Prediction: l camea de ebedeo .\n",
            "Evaluation translation everyone is happy . :  lllll lommiverppppa ?E\n",
            "-----------------------------------------------------------\n",
            "epoch 1 batch 600 loss 1.0473942756652832\n",
            "English: can he speak french ?\n",
            "Spanish Translation: ¿ l habla franc s ?\n",
            "Spaniish Prediction: ¿ l h sla pradc s ?\n",
            "Evaluation translation everyone is happy . :  lllll lizomi .E\n",
            "-----------------------------------------------------------\n",
            "epoch 1 batch 700 loss 0.7941046357154846\n",
            "English: what is on the desk ?\n",
            "Spanish Translation: ¿ qu hay en el escritorio ?\n",
            "Spaniish Prediction: ¿ qe gayoen e  escr  ara  .\n",
            "Evaluation translation everyone is happy . : poyoyo .E\n",
            "-----------------------------------------------------------\n",
            "epoch 1 batch 800 loss 0.7778571248054504\n",
            "English: tom is mary s mentor .\n",
            "Spanish Translation: tom es el mentor de mary .\n",
            "Spaniish Prediction: tom estellmenaor de .an  .\n",
            "Evaluation translation everyone is happy . :  f fir fizojo .E\n",
            "-----------------------------------------------------------\n",
            "epoch 1 batch 900 loss 1.107107162475586\n",
            "English: put it where you like .\n",
            "Spanish Translation: p nganlo donde quieran .\n",
            "Spaniish Prediction: ponaanao donde quienar .\n",
            "Evaluation translation everyone is happy . : poyyy pomomomejoy .E\n",
            "-----------------------------------------------------------\n",
            "epoch 1 batch 1000 loss 0.7799263000488281\n",
            "English: i really got depressed .\n",
            "Spanish Translation: me sent verdaderamente deprimido .\n",
            "Spaniish Prediction: me sent verda irames e ee ooeero .\n",
            "Evaluation translation everyone is happy . : poyoyo pomojo .E\n",
            "-----------------------------------------------------------\n",
            "epoch 1 batch 1100 loss 0.7952229976654053\n",
            "English: don t play with matches .\n",
            "Spanish Translation: no juegues con f sforos .\n",
            "Spaniish Prediction: no juenues co  f su  os ?\n",
            "Evaluation translation everyone is happy . :  lllllllomomes .E\n",
            "-----------------------------------------------------------\n",
            "epoch 1 batch 1200 loss 0.92210453748703\n",
            "English: we re having lunch here .\n",
            "Spanish Translation: estamos almorzando aqu .\n",
            "Spaniish Prediction: estamos almoreando esu .\n",
            "Evaluation translation everyone is happy . :  lllllllo .E\n",
            "-----------------------------------------------------------\n",
            "epoch 1 batch 1300 loss 0.9057469367980957\n",
            "English: she will be back at five .\n",
            "Spanish Translation: estar de vuelta a las cinco .\n",
            "Spaniish Prediction: estar ve vuel a a la  cenoo .\n",
            "Evaluation translation everyone is happy . : poyy fuy ,ojo .E\n",
            "-----------------------------------------------------------\n",
            "epoch 1 batch 1400 loss 0.687065601348877\n",
            "English: i want to eat some grapes .\n",
            "Spanish Translation: quiero comerme unas uvas .\n",
            "Spaniish Prediction: quiero comeree unas aeas .\n",
            "Evaluation translation everyone is happy . : poyoyopomomomeroy .E\n",
            "-----------------------------------------------------------\n",
            "epoch 1 batch 1500 loss 0.8689269423484802\n",
            "English: he is a cheerful young man .\n",
            "Spanish Translation: es un joven alegre .\n",
            "Spaniish Prediction: es un moven a egre p\n",
            "Evaluation translation everyone is happy . : po poy pomojo .E\n",
            "-----------------------------------------------------------\n",
            "epoch 1 batch 1600 loss 0.7714107036590576\n",
            "English: why did you tell this joke ?\n",
            "Spanish Translation: ¿ por qu contaste esa broma ?\n",
            "Spaniish Prediction: ¿ por quuconteste e   bror  .\n",
            "Evaluation translation everyone is happy . :  l lllllombbo .E\n",
            "-----------------------------------------------------------\n",
            "epoch 1 batch 1700 loss 0.8762733340263367\n",
            "English: they re washing their hands .\n",
            "Spanish Translation: ellos se est n lavando las manos .\n",
            "Spaniish Prediction: ellos ee est a lava  ollan .era  .\n",
            "Evaluation translation everyone is happy . : po lllomomojo .E\n",
            "-----------------------------------------------------------\n",
            "epoch 1 batch 1800 loss 0.8302294611930847\n",
            "English: i ve already eaten breakfast .\n",
            "Spanish Translation: ya he desayunado .\n",
            "Spaniish Prediction: yahhe qesayun do .\n",
            "Evaluation translation everyone is happy . :  poyoy pomobbabo .E\n",
            "-----------------------------------------------------------\n",
            "epoch 1 batch 1900 loss 0.6822549104690552\n",
            "English: i couldn t get my car started .\n",
            "Spanish Translation: no pude poner mi coche en marcha .\n",
            "Spaniish Prediction: noppude poner m  co oi en .aroao .\n",
            "Evaluation translation everyone is happy . : p poyy pomizo .E\n",
            "-----------------------------------------------------------\n",
            "epoch 1 batch 2000 loss 0.8668037056922913\n",
            "English: do you know when she will come ?\n",
            "Spanish Translation: ¿ sabes cu ndo vendr ?\n",
            "Spaniish Prediction: ¿ sabbs cu ndo vendo ?\n",
            "Evaluation translation everyone is happy . : po llopomizomer .E\n",
            "-----------------------------------------------------------\n",
            "epoch 1 batch 2100 loss 0.9069910049438477\n",
            "English: we are sorry we can t help you .\n",
            "Spanish Translation: lo sentimos mucho por no poder ayudarlos .\n",
            "Spaniish Prediction: lo sentimos mucho p   no ma o e.   o e   .\n",
            "Evaluation translation everyone is happy . : l llll lizomi .E\n",
            "-----------------------------------------------------------\n",
            "epoch 1 batch 2200 loss 0.8547016382217407\n",
            "English: the sun always sets in the west .\n",
            "Spanish Translation: el sol siempre se pone en el oeste .\n",
            "Spaniish Prediction: el sol siempra se pe t e  . on n an.\n",
            "Evaluation translation everyone is happy . : pl lllifomizo fozoy e .E\n",
            "-----------------------------------------------------------\n",
            "epoch 1 batch 2300 loss 1.0597491264343262\n",
            "English: please hurry . we re late already .\n",
            "Spanish Translation: haga el favor de apresurarse ya llegamos tarde .\n",
            "Spaniish Prediction: hasa el favorade apa sura    a  . ara a  .a  o .\n",
            "Evaluation translation everyone is happy . :  l lll lomizo fo .E\n",
            "-----------------------------------------------------------\n",
            "epoch 1 batch 2400 loss 0.801056444644928\n",
            "English: i ve been in boston since october .\n",
            "Spanish Translation: he estado desde octubre en boston .\n",
            "Spaniish Prediction: he estadd des e octoere e  .asoa  .\n",
            "Evaluation translation everyone is happy . :  l llo lomizo fo .E\n",
            "-----------------------------------------------------------\n",
            "epoch 2 batch 0 loss 1.0964490175247192\n",
            "English: go .\n",
            "Spanish Translation: ve .\n",
            "Spaniish Prediction: v  m\n",
            "Evaluation translation everyone is happy . :  l moy lomizo poz .E\n",
            "-----------------------------------------------------------\n",
            "epoch 2 batch 100 loss 0.32931509613990784\n",
            "English: she stood up .\n",
            "Spanish Translation: ella se levant .\n",
            "Spaniish Prediction: ella se levan  .\n",
            "Evaluation translation everyone is happy . : vompoy .E\n",
            "-----------------------------------------------------------\n",
            "epoch 2 batch 200 loss 0.247553750872612\n",
            "English: it s tom s job .\n",
            "Spanish Translation: es la tarea de tom .\n",
            "Spaniish Prediction: es la tarea da tom .\n",
            "Evaluation translation everyone is happy . : vomooy .E\n",
            "-----------------------------------------------------------\n",
            "epoch 2 batch 300 loss 0.35459113121032715\n",
            "English: why are you sad ?\n",
            "Spanish Translation: ¿ por qu est s triste ?\n",
            "Spaniish Prediction: ¿ por quuest a tris a ?\n",
            "Evaluation translation everyone is happy . : vomiiz mifuijopoy .E\n",
            "-----------------------------------------------------------\n",
            "epoch 2 batch 400 loss 0.31249746680259705\n",
            "English: did you know that ?\n",
            "Spanish Translation: ¿ sab as eso ?\n",
            "Spaniish Prediction: ¿ sab as eso .\n",
            "Evaluation translation everyone is happy . : vomiiz mifiz .E\n",
            "-----------------------------------------------------------\n",
            "epoch 2 batch 500 loss 0.39620158076286316\n",
            "English: he changed his job .\n",
            "Spanish Translation: l cambi de empleo .\n",
            "Spaniish Prediction: l cambi de emeleo .\n",
            "Evaluation translation everyone is happy . : vomoyovomomojopo .E\n",
            "-----------------------------------------------------------\n",
            "epoch 2 batch 600 loss 0.6015720963478088\n",
            "English: can he speak french ?\n",
            "Spanish Translation: ¿ l habla franc s ?\n",
            "Spaniish Prediction: ¿ l halla fra c s ?\n",
            "Evaluation translation everyone is happy . : vomifivomomojaboz .E\n",
            "-----------------------------------------------------------\n",
            "epoch 2 batch 700 loss 0.46833357214927673\n",
            "English: what is on the desk ?\n",
            "Spanish Translation: ¿ qu hay en el escritorio ?\n",
            "Spaniish Prediction: ¿ qu hay en e   scr  oriv ?\n",
            "Evaluation translation everyone is happy . : vompoy mbbbiz .E\n",
            "-----------------------------------------------------------\n",
            "epoch 2 batch 800 loss 0.45270150899887085\n",
            "English: tom is mary s mentor .\n",
            "Spanish Translation: tom es el mentor de mary .\n",
            "Spaniish Prediction: tom es es men oo de .ary .\n",
            "Evaluation translation everyone is happy . : vomizovomojojopojE\n",
            "-----------------------------------------------------------\n",
            "epoch 2 batch 900 loss 0.665171205997467\n",
            "English: put it where you like .\n",
            "Spanish Translation: p nganlo donde quieran .\n",
            "Spaniish Prediction: p nganlo dondo uuie an .\n",
            "Evaluation translation everyone is happy . : pompoy ,omibielpo .E\n",
            "-----------------------------------------------------------\n",
            "epoch 2 batch 1000 loss 0.4450685381889343\n",
            "English: i really got depressed .\n",
            "Spanish Translation: me sent verdaderamente deprimido .\n",
            "Spaniish Prediction: me sent verda errme te de o nieo .\n",
            "Evaluation translation everyone is happy . :  l loz lllomiejojE\n",
            "-----------------------------------------------------------\n",
            "epoch 2 batch 1100 loss 0.4714006185531616\n",
            "English: don t play with matches .\n",
            "Spanish Translation: no juegues con f sforos .\n",
            "Spaniish Prediction: no juegues co  f sfe os .\n",
            "Evaluation translation everyone is happy . : pomizopomomijopo .E\n",
            "-----------------------------------------------------------\n",
            "epoch 2 batch 1200 loss 0.5733127593994141\n",
            "English: we re having lunch here .\n",
            "Spanish Translation: estamos almorzando aqu .\n",
            "Spaniish Prediction: estamos almor ando a u .\n",
            "Evaluation translation everyone is happy . : vomizzombbbojafE\n",
            "-----------------------------------------------------------\n",
            "epoch 2 batch 1300 loss 0.6094468235969543\n",
            "English: she will be back at five .\n",
            "Spanish Translation: estar de vuelta a las cinco .\n",
            "Spaniish Prediction: estar de vuel a a la  cin u .\n",
            "Evaluation translation everyone is happy . : pomizzomomibo .E\n",
            "-----------------------------------------------------------\n",
            "epoch 2 batch 1400 loss 0.4389004707336426\n",
            "English: i want to eat some grapes .\n",
            "Spanish Translation: quiero comerme unas uvas .\n",
            "Spaniish Prediction: quiero comerm  unas .nas .\n",
            "Evaluation translation everyone is happy . : vomizo omomojabopoy .E\n",
            "-----------------------------------------------------------\n",
            "epoch 2 batch 1500 loss 0.5218132138252258\n",
            "English: he is a cheerful young man .\n",
            "Spanish Translation: es un joven alegre .\n",
            "Spaniish Prediction: es un joven a egre .\n",
            "Evaluation translation everyone is happy . : vomizovombbijabE\n",
            "-----------------------------------------------------------\n",
            "epoch 2 batch 1600 loss 0.49967825412750244\n",
            "English: why did you tell this joke ?\n",
            "Spanish Translation: ¿ por qu contaste esa broma ?\n",
            "Spaniish Prediction: ¿ por qu contaste e   bro e ?\n",
            "Evaluation translation everyone is happy . : vom ffombblojabE\n",
            "-----------------------------------------------------------\n",
            "epoch 2 batch 1700 loss 0.6420071721076965\n",
            "English: they re washing their hands .\n",
            "Spanish Translation: ellos se est n lavando las manos .\n",
            "Spaniish Prediction: ellos se est e lava  o la  .ado  .\n",
            "Evaluation translation everyone is happy . : vomizz omblojabop .E\n",
            "-----------------------------------------------------------\n",
            "epoch 2 batch 1800 loss 0.5808063745498657\n",
            "English: i ve already eaten breakfast .\n",
            "Spanish Translation: ya he desayunado .\n",
            "Spaniish Prediction: ya he desayun ro .\n",
            "Evaluation translation everyone is happy . : vo foz ombblo .E\n",
            "-----------------------------------------------------------\n",
            "epoch 2 batch 1900 loss 0.5206567049026489\n",
            "English: i couldn t get my car started .\n",
            "Spanish Translation: no pude poner mi coche en marcha .\n",
            "Spaniish Prediction: no pude poner mi co ue en .artoo .\n",
            "Evaluation translation everyone is happy . : po fiicomblojabo .E\n",
            "-----------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "transformer.train() # used to set the model in training mode\n",
        "total_loss = 0\n",
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "  iterator = iter(train_loader)\n",
        "  '''\n",
        "  looping on iterator we will get the batch of input_sequence and target_sequence\n",
        "  [(batch_of_input_sequence),(batch_of_target_sequence)]\n",
        "  '''\n",
        "\n",
        "  for batch_num,batch  in enumerate(iterator):\n",
        "    '''\n",
        "    batch_num{int}: current batch number\n",
        "    batch{list{tuple}}: batch of input_sequence and target_sequence\n",
        "    '''\n",
        "    transformer.train()\n",
        "    eng_batch,sp_batch = batch\n",
        "    # creating the mask for the eng_batch and sp_batch\n",
        "    encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(eng_batch, sp_batch)\n",
        "    '''\n",
        "    ***important***\n",
        "    {the gradients of each mini-batch should be computed independantly}\n",
        "\n",
        "    .zero-grad():set gradient to zero at the start of training of each mini-batch\n",
        "                 so that while backpropogation the gradient is not accumulated:\n",
        "                              (means gradient of these mini batch would not effect the gradient of other mini-batch)\n",
        "\n",
        "    '''\n",
        "    optim.zero_grad()\n",
        "\n",
        "    sp_predictions = transformer(eng_batch,\n",
        "                                 sp_batch,\n",
        "                                 encoder_self_attention_mask,\n",
        "                                 decoder_self_attention_mask,\n",
        "                                 decoder_cross_attention_mask,\n",
        "                                 enc_start_token = False,\n",
        "                                 enc_end_token = False,\n",
        "                                 dec_start_token = True,\n",
        "                                 dec_end_token = True)\n",
        "    '''\n",
        "    labels{tensor}:\n",
        "          converting spanish sentences into their index values based on spanish_to_index\n",
        "    '''\n",
        "\n",
        "    labels = transformer.decoder.sentence_embedding.batch_tokenize(sp_batch, start_token=False, end_token=True) # shspe (batch_size,max_sequence_length) {max_sequence_length  = num_queries}\n",
        "  \n",
        "    '''\n",
        "    loss = criterian(....):\n",
        "        represent loss of each mini-batch\n",
        "        by computimg loss over all characters(or tokens) in a batch of sentences\n",
        "        loss[:max_sequnce_length] = loss of 1st sentence of batch\n",
        "    '''\n",
        "    loss = criterian(\n",
        "        sp_predictions.view(-1,sp_vocab_size).to(device), # shape (batch_size * num_queries,sp_vocab_size)\n",
        "        labels.view(-1).to(device) # shape (bach_size * num_queries.)\n",
        "    ) # shape (batch_size * num_queries)\n",
        "  \n",
        "\n",
        "    '''\n",
        "    valid_indices:\n",
        "                setting true value where labels are not padding token\n",
        "                and false value where labels are padding token\n",
        "    '''\n",
        "    valid_indicies = torch.where(labels.view(-1) == spanish_to_index[PADDING_TOKEN], False, True) # shape (batch_size * num_queries)\n",
        "    # print(valid_indicies[:50])\n",
        "    # print(loss.sum())\n",
        "    # print(valid_indicies.sum())\n",
        "    '''\n",
        "    loss = loss.sum()/valid...:\n",
        "                represent a loss value(single number), where the loss of all padding tokens are ignored\n",
        "\n",
        "    '''\n",
        "    loss = loss.sum() / valid_indicies.sum()\n",
        "\n",
        "\n",
        "    '''\n",
        "    loss.backward():\n",
        "                compute gradient or derivative using the loss function and model's parameters\n",
        "                *** Theory ***\n",
        "                          L = loss function\n",
        "                          w = model's weight or pparameter\n",
        "                          dL/dw = gradient of loss function wrt weight w\n",
        "                          dL/dw:\n",
        "                                represent how the loss function is changing if we change the model's parameter w(increase or decrease w value)\n",
        "\n",
        "                                computed using chain rule of calculus\n",
        "\n",
        "    optim.step():\n",
        "              updating the model parameter with above computed gradient or derivative\n",
        "              using a specific optmizer equation{Adam,Gradient descent,SGD,...}\n",
        "\n",
        "    .item():\n",
        "            return the value of torch tensor which containe only one value\n",
        "\n",
        "    '''\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "\n",
        "    if (batch_num % 100 == 0):\n",
        "      print(f\"epoch {epoch} batch {batch_num} loss {loss.item()}\")\n",
        "      print(f\"English: {eng_batch[0]}\")\n",
        "      print(f\"Spanish Translation: {sp_batch[0]}\")\n",
        "      '''\n",
        "      sp_predictions[0]:\n",
        "                  represent the output of transformer for a first trainable example in a batch\n",
        "      sp_sentence_predicted:\n",
        "                  represent the predicted index of spanish sentence for a first trainable example in a batch\n",
        "      '''\n",
        "\n",
        "      sp_sentence_predicted = torch.argmax(sp_predictions[0], # shape (num_queries,sp_vocab_size)\n",
        "                                           axis=1)  # shape (num_queries,)\n",
        "\n",
        "      predicted_sentence = \"\"\n",
        "      for idx in sp_sentence_predicted:\n",
        "        # print(idx.item())\n",
        "        if idx == spanish_to_index[END_TOKEN]:\n",
        "          break\n",
        "        predicted_sentence += index_to_spanish[idx.item()]\n",
        "      print(f'Spaniish Prediction: {predicted_sentence}')\n",
        "\n",
        "\n",
        "      transformer.eval()  # used to set the model in evaluation mode\n",
        "      '''\n",
        "      ***Important***\n",
        "         a =  ('dksdkk')\n",
        "         type(a),len(a)\n",
        "         --> str, 6\n",
        "         a =  ('dksdkk',)\n",
        "         type(a),len(a)\n",
        "         ---> tuple, 1\n",
        "      '''\n",
        "      sp_sentence = (\"\",)\n",
        "      eng_sentence  = [\"everyone is happy.\"]\n",
        "      for i,sentence in enumerate(eng_sentence):\n",
        "        eng_sentence[i] = text_preprocessing(sentence)\n",
        "      eng_sentence = tuple(eng_sentence)\n",
        "      for word_counter in range(max_sequence_length):\n",
        "          encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask= create_masks(eng_sentence, sp_sentence)\n",
        "          predictions = transformer(eng_sentence,\n",
        "                                          sp_sentence,\n",
        "                                          encoder_self_attention_mask,\n",
        "                                          decoder_self_attention_mask,\n",
        "                                          decoder_cross_attention_mask,\n",
        "                                          enc_start_token=False,\n",
        "                                          enc_end_token=False,\n",
        "                                          dec_start_token=True,\n",
        "                                          dec_end_token=False)\n",
        "      \n",
        "          '''\n",
        "          next_token_:\n",
        "                  contains raw prediction values for each token in vocabulary\n",
        "                  example:\n",
        "                        \"<start>i am happy<end>\"\n",
        "                        input token <start> feeded to transformer decoder\n",
        "                        then output of transformer represent raw prediction values,\n",
        "                        predicting what could be the next word (after <start> token) form the vocabulary\n",
        "                        shape (sp_vocab_size,)\n",
        "\n",
        "                                  I    <end>   a      m     h\n",
        "                        output = [21.5 , 7.9  , 0.1 , -20 , -2.4 ,...] {random_values}\n",
        "                        highest prediction value is the next word\n",
        "\n",
        "                        {real raw values range depnds on type activation function that is used}\n",
        "\n",
        "\n",
        "          '''\n",
        "          next_token_raw_distribution = predictions[0][word_counter] # shape (sp_vocab_size,)\n",
        "          next_token_index = torch.argmax(next_token_raw_distribution).item()\n",
        "          next_token = index_to_spanish[next_token_index]\n",
        "          sp_sentence = (sp_sentence[0] + next_token,)\n",
        "          if (next_token == END_TOKEN):\n",
        "            break\n",
        "\n",
        "      print(f'Evaluation translation {eng_sentence[0]} : {sp_sentence[0]}')\n",
        "      print(f'-----------------------------------------------------------')\n",
        "\n",
        "\n",
        "    # break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaGJriaTNIRo"
      },
      "outputs": [],
      "source": [
        "transformer.eval()\n",
        "def translate(input_sentence):\n",
        "  '''\n",
        "      ***Important***\n",
        "         a =  ('dksdkk')\n",
        "         type(a),len(a)\n",
        "         --> str, 6\n",
        "         a =  ('dksdkk',)\n",
        "         type(a),len(a)\n",
        "         ---> tuple, 1\n",
        "      '''\n",
        "  sp_sentence = (\"\",)\n",
        "  eng_sentence  = [input_sentence]\n",
        "  for i,sentence in enumerate(eng_sentence):\n",
        "      eng_sentence[i] = text_preprocessing(sentence)\n",
        "  eng_sentence = tuple(eng_sentence)\n",
        "  for word_counter in range(max_sequence_length):\n",
        "      encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask= create_masks(eng_sentence, sp_sentence)\n",
        "      predictions = transformer(eng_sentence,\n",
        "                                          sp_sentence,\n",
        "                                          encoder_self_attention_mask,\n",
        "                                          decoder_self_attention_mask,\n",
        "                                          decoder_cross_attention_mask,\n",
        "                                          enc_start_token=False,\n",
        "                                          enc_end_token=False,\n",
        "                                          dec_start_token=True,\n",
        "                                          dec_end_token=False)\n",
        "   \n",
        "      '''\n",
        "      next_token_:\n",
        "              contains raw prediction values for each token in vocabulary\n",
        "              example:\n",
        "                    \"<start>i am happy<end>\"\n",
        "                    input token <start> feeded to transformer decoder\n",
        "                    then output of transformer represent raw prediction values,\n",
        "                    predicting what could be the next word (after <start> token) form the vocabulary\n",
        "                    shape (sp_vocab_size,)\n",
        "\n",
        "                              I    <end>   a      m     h\n",
        "                    output = [21.5 , 7.9  , 0.1 , -20 , -2.4 ,...] {random_values}\n",
        "                    highest prediction value is the next word\n",
        "\n",
        "                    {real raw values range depnds on type activation function that is used}\n",
        "\n",
        "\n",
        "      '''\n",
        "      next_token_raw_distribution = predictions[0][word_counter] # shape (sp_vocab_size,)\n",
        "      # print(next_token_raw_distribution)\n",
        "      next_token_index = torch.argmax(next_token_raw_distribution).item()\n",
        "      # print(next_token_index)\n",
        "      next_token = index_to_spanish[next_token_index]\n",
        "      # print(next_token)\n",
        "      sp_sentence = (sp_sentence[0] + next_token,)\n",
        "      if (next_token == END_TOKEN):\n",
        "        break\n",
        "\n",
        "  print(f'Evaluation translation {eng_sentence[0]} : {sp_sentence[0]}')\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
